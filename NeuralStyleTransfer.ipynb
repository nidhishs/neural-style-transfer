{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ap5V0GyKLjEx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "import torch.optim as optim\n",
        "from torchvision.utils import save_image\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zn1GTrQ-jk2B"
      },
      "source": [
        "## Image Preprocessing\n",
        "\n",
        "Before feeding the images into any Computer Vision model, we require preprocessing of the images. The most frequently used preprocessing techniques are:\n",
        "- `transforms.Resize(256)` : Resize all images to size (256, 256).\n",
        "- `transforms.ToTensor()` : Converts a PIL Image to a PyTorch tensor (multi-dimensional array)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_ZiFlgnhvNm"
      },
      "outputs": [],
      "source": [
        "def image_loader(img_path):\n",
        "    \"\"\"Given an image path, it returns the image tensor\"\"\"\n",
        "\n",
        "    image=Image.open(img_path).convert('RGB')\n",
        "    # TODO : Implement preprocessing transform.\n",
        "    # transform = \n",
        "    image = transform(image).unsqueeze(0)\n",
        "    return image.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXGcWhc2n_Kh"
      },
      "source": [
        "## Loading and Visualising Images\n",
        "\n",
        "Now we can use the `image_loader` function to load in the Style and Content images. The `image_plotter` function can then be utilised to visualise all the images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdHsO3oxnvi3"
      },
      "outputs": [],
      "source": [
        "# TODO: Load in the style and content images.\n",
        "# style_image = image_loader()\n",
        "# content_image = image_loader()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PjNq7ojHiM-2"
      },
      "outputs": [],
      "source": [
        "def image_plotter(imgs):\n",
        "    \"\"\"Given an array of image tensors, it plots them in a grid\"\"\"\n",
        "    to_PIL = transforms.ToPILImage()\n",
        "    nrows = int(np.sqrt(len(imgs)))\n",
        "    ncols = int(np.ceil(len(imgs)/nrows))\n",
        "    fig, axes = plt.subplots(nrows, ncols, figsize=(ncols*8, nrows*8))\n",
        "    for i, ax in enumerate(axes.flat):\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "        if i < len(imgs):\n",
        "            img = to_PIL(imgs[i].squeeze(0))\n",
        "            ax.imshow(img)\n",
        "    plt.subplots_adjust(wspace=0.005, hspace=0.005)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmAfCsD8nssA"
      },
      "outputs": [],
      "source": [
        "image_plotter([content_image, style_image])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5Jf9ADps5oG"
      },
      "source": [
        "## Model definition\n",
        "\n",
        "Neural Style Transfer can applied using any Computer Vision model, however we replicate the [paper](https://arxiv.org/abs/1508.06576) by using a VGG-19 model pretrained on the ImageNet dataset.\n",
        "\n",
        "We will use the feature representations of the layers indexed by `[0, 5, 10, 19, 28]`, namely, the `conv1_1`, `conv2_1`, `conv3_1`, `conv4_1` and `conv5_1` layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tzOe6u9Et65E"
      },
      "outputs": [],
      "source": [
        "class VGG(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VGG, self).__init__()\n",
        "        # TODO: Define selected indices and the base model\n",
        "        # self.features = \n",
        "        #self.model = \n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO: Accumulate representations from the selected layers.\n",
        "        representations = []\n",
        "        \n",
        "        return representations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhI-PWOKxJMl"
      },
      "source": [
        "## Loss function definition\n",
        "\n",
        "The loss function for neural style transfer is defined as:\n",
        "$$\\mathcal{L}(\\vec{G}, \\vec{C}, \\vec{S}) = \\alpha\\mathcal{L}_c(\\vec{G}, \\vec{C}) + \\beta\\mathcal{L}_s(\\vec{G}, \\vec{S})$$\n",
        "where,\n",
        "- $\\mathcal{L}(\\vec{G}, \\vec{C}, \\vec{S})$ is the total loss of the generated image $\\vec{G}$ with respect to the content $\\vec{C}$ and style $\\vec{S}$ images.\n",
        "- $\\mathcal{L}_c(\\vec{G}, \\vec{C})$ is the content loss, which measures how similar the content image $\\vec{C}$ is to the generated image $\\vec{G}$.\n",
        "- $\\mathcal{L}_s(\\vec{G}, \\vec{S})$ is the style loss, which measures how similar the style image $\\vec{S}$ is to the generated image $\\vec{G}$.\n",
        "- $\\alpha$ and $\\beta$ are hyper-parameters to be defined later.\n",
        "\n",
        "To optimize the model parameters, traditional machine learning uses gradient descent on the loss function. In Neural Style Transfer, on the other hand, we use the loss to optimize the pixel values of the input image. We can make the input image emphasize either the style or the content image by choosing suitable values for $\\alpha$ and $\\beta$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X47sHhTV5AVN"
      },
      "source": [
        "### Content Loss\n",
        "\n",
        "The content loss is the mean squared error (MSE) between the representations of the content $\\vec{C}$ and the generated $\\vec{G}$ image at the $l^{th}$ layer, and can be defined as follows:\n",
        "$$\\mathcal{L}_c(\\vec{G}, \\vec{C}, l) = \\frac{1}{2}\\left\\lVert C^{[l]} - G^{[l]}\\right\\rVert ^ 2$$\n",
        "where,\n",
        "- $C^{[l]}$ is the activation of the $l^{th}$ layer on the content image $\\vec{C}$.\n",
        "- $G^{[l]}$ is the activation of the $l^{th}$ layer on the generated image $\\vec{G}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TX-kwEvbupo2"
      },
      "outputs": [],
      "source": [
        "def content_loss(G, C):\n",
        "    loss = F.mse_loss(G, C)\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDghOyUo7tRr"
      },
      "source": [
        "### Style Loss\n",
        "\n",
        "Similarly, the style loss is the mean squared error (MSE) betwen the gram matrices of the representation of the style $\\vec{S}$ and the generated $\\vec{G}$ images at the $l^{th}$ layer.\n",
        "\n",
        "$$\\mathcal{L}_s(\\vec{G}, \\vec{S}, l) = \\left( \\frac{1}{2n^{[l]}_Cn^{[l]}_Hn^{[l]}_W} \\right)^2 \\left\\lVert C^{[l]}_{gram} - G^{[l]}_{gram}\\right\\rVert ^ 2$$\n",
        "where,\n",
        "- $C^{[l]}_{gram}$ and is the Gram Matrix of content image $\\vec{C}$ at the $l^{th}$ layer.\n",
        "- $G^{[l]}_{gram}$ and is the Gram Matrix of generated image $\\vec{G}$ at the $l^{th}$ layer.\n",
        "- $n^{[l]}_C$, $n^{[l]}_H$, $n^{[l]}_W$ are the number of channels, height and width respectively of the image at the $l^{th}$ layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKXrLuSm7sEi"
      },
      "outputs": [],
      "source": [
        "def style_loss(G, S):\n",
        "\n",
        "    def gram_matrix(a):\n",
        "        B, C, H, W = a.shape\n",
        "        A = a.reshape(B*C, H*W)\n",
        "        return torch.matmul(A, A.t()).div(B*C*H*W)\n",
        "\n",
        "    S_gram = gram_matrix(S)\n",
        "    G_gram = gram_matrix(G)\n",
        "\n",
        "    return F.mse_loss(G_gram, S_gram)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsj2cgf481-N"
      },
      "source": [
        "### Total Loss\n",
        "As defined previously, the total loss function for neural style transfer is defined as:\n",
        "$$\\mathcal{L}(\\vec{G}, \\vec{C}, \\vec{S}) = \\alpha\\mathcal{L}_c(\\vec{G}, \\vec{C}) + \\beta\\mathcal{L}_s(\\vec{G}, \\vec{S})$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xj1rJYQmDFP0"
      },
      "outputs": [],
      "source": [
        "def total_loss(G, C, S, alpha=1, beta=1e8):\n",
        "    style_loss_val = 0\n",
        "    content_loss_val = 0\n",
        "\n",
        "    # TODO: Calculate the style and content loss for \n",
        "\n",
        "    for g, c, s in zip(G, C, S):\n",
        "        content_loss_val += content_loss(g, c)\n",
        "        style_loss_val += style_loss(g, s)\n",
        "\n",
        "    total_loss = alpha*content_loss_val + beta*style_loss_val\n",
        "\n",
        "    return total_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChfUyCakLdUp"
      },
      "source": [
        "## Inference\n",
        "\n",
        "Now given a style image and a content image, we can begin from white noise to generate our neural style transfer image. However, cloning the content image as the initial generated image usually gives better results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2MBSRLhULfaL"
      },
      "outputs": [],
      "source": [
        "def style_transfer(G, C, S, model, epochs, loss_function, optimizer):\n",
        "    tic = time.time()\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        # TODO: Pass in G, C, S through the model and calculate and back prop the loss.\n",
        "\n",
        "        if epoch%200==0 or epoch==epochs-1:\n",
        "            toc = time.time()\n",
        "            print(f'Epoch: {epoch+1:4d}/{epochs} | Total Loss: {torch.mean(loss):.3f} | Time: {toc-tic:.2f}s')\n",
        "            save_image(G.squeeze(0), 'generated_image.jpeg')\n",
        "            tic = time.time()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xGQz1Z1NmGm"
      },
      "source": [
        "We can run the inference loop for 1000 epochs. It's worth noting that we usually pass the model parameters to the optimizer, but in this case, because we want to optimize the image, we pass the input white noise image to the optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7PAJ7xJGLcoc"
      },
      "outputs": [],
      "source": [
        "# TODO: Clone the content image\n",
        "# generated_image = "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLGpwEiLSh7T"
      },
      "outputs": [],
      "source": [
        "# The model is set to evaluation mode since we are not training the model.\n",
        "nst_model = VGG().to(device).eval()\n",
        "optimizer = optim.Adam([generated_image], lr=0.03)\n",
        "\n",
        "style_transfer(generated_image, content_image, style_image, nst_model, 4000, total_loss, optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dhx6UeZXWf1i"
      },
      "outputs": [],
      "source": [
        "final_image = image_loader('generated_image.jpeg')\n",
        "image_plotter([final_image, content_image, style_image])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "NeuralStyleTransfer.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
